# modules/yolo_detector.py - ê¸°ì¡´ API ìœ ì§€í•˜ë©´ì„œ ê°œì„ 

import cv2
import numpy as np
from ultralytics import YOLO
import os
import shutil
from collections import Counter
from io import BytesIO
from PIL import Image as PILImage
import re
import json

# ===== ğŸ”§ JSON ì„¤ì • ë¡œë” (ê¸°ì¡´ ìœ ì§€) =====
def load_json_config(config_file, default_data):
    """JSON ì„¤ì • íŒŒì¼ ë¡œë“œ ë˜ëŠ” ê¸°ë³¸ê°’ ìƒì„±"""
    try:
        if os.path.exists(config_file):
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        else:
            os.makedirs(os.path.dirname(config_file), exist_ok=True)
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(default_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ“ ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„±: {config_file}")
            return default_data
    except Exception as e:
        print(f"âŒ ì„¤ì • íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({config_file}): {e}")
        return default_data

# ===== ğŸ”§ ì„¤ì • íŒŒì¼ë“¤ ë¡œë“œ (ê¸°ì¡´ ìœ ì§€) =====
brand_config = load_json_config("config/brand_mapping.json", {"brand_mappings": {}})
BRAND_MAPPINGS = brand_config.get('brand_mappings', {})

korean_config = load_json_config("config/korean_class_mapping.json", {"korean_class_mapping": {}})
KOREAN_CLASS_MAPPING = korean_config.get('korean_class_mapping', {})

color_config = load_json_config("config/color_food_mapping.json", {"color_ranges": {}, "correction_rules": []})
COLOR_RANGES = color_config.get('color_ranges', {})

# ğŸ†• ì¼ë°˜í™”ëœ ë³´ì • ê·œì¹™ ë¡œë“œ
correction_config = load_json_config("config/correction_rules.json", {
    "correction_rules": [
        {
            "name": "orange_correction",
            "conditions": {
                "detected_class": ["potato"],
                "color_match": ["ì§„í•œì£¼í™©ìƒ‰", "ì—°í•œì£¼í™©ìƒ‰", "ì£¼í™©ë¹¨ê°„ìƒ‰"],
                "confidence_threshold": 0.85
            },
            "actions": {
                "new_class": "orange",
                "new_korean_name": "ì˜¤ë Œì§€",
                "confidence_boost": 0.1,
                "max_confidence": 0.92
            }
        }
    ]
})
CORRECTION_RULES = correction_config.get('correction_rules', [])

print(f"âœ… ì„¤ì • ë¡œë“œ ì™„ë£Œ: ë¸Œëœë“œ {len(BRAND_MAPPINGS)}ê°œ, í•œêµ­ì–´ {len(KOREAN_CLASS_MAPPING)}ê°œ, ë³´ì •ê·œì¹™ {len(CORRECTION_RULES)}ê°œ")

# ===== ëª¨ë¸ ìºì‹œ ì‹œìŠ¤í…œ (ê¸°ì¡´ ìœ ì§€) =====
_model_cache = {}

def get_cached_model(model_path):
    """ìºì‹œëœ ëª¨ë¸ ë°˜í™˜ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)"""
    if model_path not in _model_cache:
        _model_cache[model_path] = load_yolo_model(model_path)
    return _model_cache[model_path]

def clear_model_cache():
    """ëª¨ë¸ ìºì‹œ ì´ˆê¸°í™”"""
    global _model_cache
    _model_cache.clear()
    print("ğŸ—‘ï¸ ëª¨ë¸ ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")

# ===== ê¸°ì¡´ í•¨ìˆ˜ë“¤ ìœ ì§€ =====
def load_yolo_model(model_path=None):
    """YOLO ëª¨ë¸ ë¡œë“œ - ê¸°ì¡´ ê·¸ëŒ€ë¡œ"""

    if model_path is None:
        model_path = "models/yolo11s.pt"
    
    try:
        if not os.path.exists(model_path):
            print(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
            os.makedirs("models", exist_ok=True)
            
            if "yolo11s.pt" in model_path:
                print("ğŸ“¥ YOLO11s ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                model = YOLO('yolo11s.pt')
                if os.path.exists('yolo11s.pt'):
                    shutil.move('yolo11s.pt', model_path)
                    print(f"âœ… ëª¨ë¸ì„ {model_path}ë¡œ ì´ë™ì™„ë£Œ")
                return model
            else:
                print(f"âš ï¸ {model_path} íŒŒì¼ì„ ìˆ˜ë™ìœ¼ë¡œ models/ í´ë”ì— ë°°ì¹˜í•´ì£¼ì„¸ìš”")
                return None
        
        print(f"ğŸ“¦ YOLO ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")
        model = YOLO(model_path)
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ: {model_path}")
        
        return model
        
    except Exception as e:
        print(f"âŒ YOLO ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

def detect_objects(model, image_path, confidence_threshold=0.5, save_result=False):
    """ê¸°ë³¸ YOLO ê°ì²´ íƒì§€ - ê¸°ì¡´ ê·¸ëŒ€ë¡œ"""
    if model is None:
        raise ValueError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
    
    try:
        print(f"ğŸ” ê°ì²´ íƒì§€ ì‹œì‘: {image_path}")
        print(f"   ì‹ ë¢°ë„ ì„ê³„ê°’: {confidence_threshold}")
        
        results = model(image_path, conf=confidence_threshold, verbose=False)
        
        detections = []
        result_image_path = None
        
        for i, result in enumerate(results):
            boxes = result.boxes
            
            if boxes is not None and len(boxes) > 0:
                print(f"   íƒì§€ëœ ê°ì²´ ìˆ˜: {len(boxes)}")
                
                for j, box in enumerate(boxes):
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    confidence = float(box.conf[0].cpu().numpy())
                    class_id = int(box.cls[0].cpu().numpy())
                    class_name = model.names[class_id]
                    korean_name = KOREAN_CLASS_MAPPING.get(class_name, class_name)
                    
                    detection = {
                        'id': j,
                        'class': class_name,
                        'korean_name': korean_name,
                        'confidence': round(confidence, 3),
                        'bbox': [float(x1), float(y1), float(x2), float(y2)],
                        'center': [float((x1 + x2) / 2), float((y1 + y2) / 2)],
                        'width': float(x2 - x1),
                        'height': float(y2 - y1),
                        'area': float((x2 - x1) * (y2 - y1))
                    }
                    
                    detections.append(detection)
                    
                    print(f"      {j+1}. {korean_name}({class_name}) - ì‹ ë¢°ë„: {confidence:.3f}")
                
                if save_result:
                    try:
                        result_dir = "results"
                        os.makedirs(result_dir, exist_ok=True)
                        
                        base_name = os.path.splitext(os.path.basename(image_path))[0]
                        result_image_path = os.path.join(result_dir, f"{base_name}_detected.jpg")
                        
                        annotated_img = result.plot()
                        cv2.imwrite(result_image_path, annotated_img)
                        print(f"ğŸ“¸ ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥: {result_image_path}")
                        
                    except Exception as save_error:
                        print(f"âš ï¸ ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {save_error}")
                        result_image_path = None
            else:
                print("   íƒì§€ëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        print(f"âœ… íƒì§€ ì™„ë£Œ: ì´ {len(detections)}ê°œ ê°ì²´")
        
        return detections, result_image_path
        
    except Exception as e:
        print(f"âŒ ê°ì²´ íƒì§€ ì˜¤ë¥˜: {e}")
        raise

def filter_food_detections(detections, food_keywords=None):
    """ìŒì‹ ê´€ë ¨ íƒì§€ ê²°ê³¼ë§Œ í•„í„°ë§ - ê¸°ì¡´ ê·¸ëŒ€ë¡œ"""
    if food_keywords is None:
        food_keywords = [
            'apple', 'banana', 'orange', 'strawberry', 'grape', 'watermelon',
            'pineapple', 'peach', 'pear', 'kiwi', 'lemon', 'lime', 'mango',
            'cherry', 'plum', 'carrot', 'broccoli', 'cabbage', 'lettuce', 'spinach',
            'onion', 'garlic', 'potato', 'tomato', 'cucumber', 'pepper', 'corn',
            'mushroom', 'eggplant', 'radish', 'bean', 'pumpkin',
            'beef', 'pork', 'chicken', 'fish', 'salmon', 'tuna', 'shrimp',
            'crab', 'lobster', 'squid', 'octopus', 'egg',
            'milk', 'cheese', 'butter', 'yogurt', 'cream',
            'rice', 'bread', 'noodle', 'pasta', 'cereal',
            'water', 'juice', 'coffee', 'tea', 'soda', 'beer', 'wine',
            'salt', 'sugar', 'oil', 'honey', 'jam', 'nut', 'beet'
        ]
    
    food_detections = []
    
    for detection in detections:
        class_name = detection['class'].lower()
        
        if any(keyword in class_name for keyword in food_keywords) or \
           detection['class'] in KOREAN_CLASS_MAPPING:
            food_detections.append(detection)
    
    print(f"ğŸ ìŒì‹ ê´€ë ¨ ê°ì²´ í•„í„°ë§: {len(detections)} â†’ {len(food_detections)}ê°œ")
    
    return food_detections

def merge_similar_detections(detections, iou_threshold=0.5):
    """ìœ ì‚¬í•œ ìœ„ì¹˜ì˜ íƒì§€ ê²°ê³¼ë“¤ì„ ë³‘í•© - ê¸°ì¡´ ê·¸ëŒ€ë¡œ"""
    if len(detections) <= 1:
        return detections
    """IoU ê³„ì‚° í•¨ìˆ˜"""
    def calculate_iou(box1, box2):
        """IoU ê³„ì‚°"""
        x1_1, y1_1, x2_1, y2_1 = box1
        x1_2, y1_2, x2_2, y2_2 = box2
        
        x1_inter = max(x1_1, x1_2)
        y1_inter = max(y1_1, y1_2)
        x2_inter = min(x2_1, x2_2)
        y2_inter = min(y2_1, y2_2)
        
        if x2_inter <= x1_inter or y2_inter <= y1_inter:
            return 0.0
        
        inter_area = (x2_inter - x1_inter) * (y2_inter - y1_inter)
        
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union_area = area1 + area2 - inter_area
        
        return inter_area / union_area if union_area > 0 else 0.0
    
    sorted_detections = sorted(detections, key=lambda x: x['confidence'], reverse=True)
    merged_detections = []
    
    for current in sorted_detections:
        merged = False
        
        for i, existing in enumerate(merged_detections):
            iou = calculate_iou(current['bbox'], existing['bbox'])
            # í´ë˜ìŠ¤ ê°™ê³  IoU ê¸°ì¤€ ì¶©ì¡±ì‹œ ë³‘í•©
            if current['class'] == existing['class'] and  iou > iou_threshold:
                if current['confidence'] > existing['confidence']:
                    merged_detections[i] = current
                merged = True
                break
            elif iou > 0.75:
                print(f"âš ï¸ í´ë˜ìŠ¤ ë‹¤ë¥´ì§€ë§Œ IoU {iou:.2f} â†’ ë³‘í•© ì‹œë„: {existing['class']} â†’ {current['class']}")
                if current['confidence'] > existing['confidence']:
                    merged_detections[i] = current
                merged = True
                break
        # 25.06.08 ê²¹ì¹˜ëŠ” ì˜ì—­ì´ ìˆìœ¼ë©´ í´ë˜ìŠ¤ ë‹¬ë¼ë„ ë®ì–´ì“°ê¸° ë³‘í•© ì‹œë„ 
        if not merged:
            merged_detections.append(current)
    
    print(f"ğŸ”„ ì¤‘ë³µ ì œê±°: {len(detections)} â†’ {len(merged_detections)}ê°œ")
    
    return merged_detections

def analyze_detection_results(detections):
    """íƒì§€ ê²°ê³¼ ë¶„ì„ ë° í†µê³„ ìƒì„± - ê¸°ì¡´ ê·¸ëŒ€ë¡œ"""
    if not detections:
        return {
            'total_objects': 0,
            'unique_classes': 0,
            'class_counts': {},
            'confidence_stats': {},
            'size_distribution': {}
        }
    
    class_counts = {}
    confidences = []
    sizes = []
    
    for detection in detections:
        class_name = detection['korean_name']
        class_counts[class_name] = class_counts.get(class_name, 0) + 1
        confidences.append(detection['confidence'])
        sizes.append(detection['area'])
    
    confidence_stats = {
        'average': round(np.mean(confidences), 3),
        'min': round(np.min(confidences), 3),
        'max': round(np.max(confidences), 3),
        'std': round(np.std(confidences), 3)
    }
    
    size_stats = {
        'average': round(np.mean(sizes), 2),
        'min': round(np.min(sizes), 2),
        'max': round(np.max(sizes), 2),
        'std': round(np.std(sizes), 2)
    }
    
    analysis = {
        'total_objects': len(detections),
        'unique_classes': len(class_counts),
        'class_counts': class_counts,
        'confidence_stats': confidence_stats,
        'size_distribution': size_stats,
        'most_common_class': max(class_counts.items(), key=lambda x: x[1]) if class_counts else None,
        'highest_confidence': max(detections, key=lambda x: x['confidence']) if detections else None
    }
    
    return analysis

# ===== ğŸš€ Enhanced YOLO Detector - ê°œì„ ëœ ë²„ì „ =====

class EnhancedYOLODetector:
    """í–¥ìƒëœ YOLO íƒì§€ê¸° - ì¼ë°˜í™”ëœ ë³´ì • ê·œì¹™ ì ìš©"""
    
    def __init__(self, models_dict=None):
        self.models = models_dict or {}
        self.brand_mappings = BRAND_MAPPINGS
        self.correction_rules = CORRECTION_RULES  # ğŸ†• ì¼ë°˜í™”ëœ ë³´ì • ê·œì¹™
        print(f"ğŸš€ Enhanced YOLO ì´ˆê¸°í™”: {len(self.models)}ê°œ ëª¨ë¸, {len(self.brand_mappings)}ê°œ ë¸Œëœë“œ, {len(self.correction_rules)}ê°œ ë³´ì • ê·œì¹™")
    def detect_objects(self, image_input, confidence=0.5):
        """EnhancedYOLODetectorìš© ê°ì²´ íƒì§€ ë©”ì„œë“œ"""
        try:
            # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            if isinstance(image_input, str):
                # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš°
                image_path = image_input
            else:
                # ì´ë¯¸ì§€ ê°ì²´ì¸ ê²½ìš° ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                import tempfile
                import cv2
                with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:
                    cv2.imwrite(tmp.name, image_input)
                    image_path = tmp.name
            
            # ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ íƒì§€
            if not self.models:
                return [], None
            
            model_name, model = next(iter(self.models.items()))
            print(f"ğŸ” ì±„ì†Œ ë¶„ì„ìš© íƒì§€ (ëª¨ë¸: {model_name})")
            
            # ì „ì—­ detect_objects í•¨ìˆ˜ ì‚¬ìš©
            detections, result_image = detect_objects(model, image_path, confidence)
            
            return detections, result_image
            
        except Exception as e:
            print(f"âŒ EnhancedYOLODetector.detect_objects ì‹¤íŒ¨: {e}")
            return [], None
    
    def _create_vegetable_confidence_score(self, predictions, color_analysis, shape_analysis):
        """ì±„ì†Œ ì˜ˆì¸¡ ì‹ ë¢°ë„ ê³„ì‚°"""
        try:
            if not predictions:
                return 0.5
            
            base_confidence = 0.7
            
            # ìƒ‰ìƒ ê¸°ë°˜ ë³´ì •
            primary_color = color_analysis.get("primary_color", "")
            if primary_color in ["ì§„í•œì´ˆë¡ìƒ‰", "ì¤‘ê°„ì´ˆë¡ìƒ‰", "ì—°í•œì´ˆë¡ìƒ‰"]:
                base_confidence += 0.1
            
            # ëª¨ì–‘ ê¸°ë°˜ ë³´ì •  
            primary_shape = shape_analysis.get("primary_shape", "")
            if primary_shape in ["ë¶ˆê·œì¹™í˜•", "ì›í˜•"]:
                base_confidence += 0.05
            
            return min(base_confidence, 0.95)
            
        except Exception as e:
            print(f"âš ï¸ ì‹ ë¢°ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.7
    def analyze_ocr_first(self, ocr_text):
        """OCR ìš°ì„  ë¸Œëœë“œ ì¸ì‹ - ê¸°ì¡´ ê·¸ëŒ€ë¡œ"""
        if not ocr_text or not ocr_text.strip():
            return None
        
        print(f"ğŸ” ë¸Œëœë“œ ì¸ì‹ ì‹œì‘: '{ocr_text}'")
        
        normalized_text = ocr_text.lower().strip()
        
        for category, data in self.brand_mappings.items():
            for brand in data["brands"]:
                if brand.lower() in normalized_text:
                    print(f"âœ… ë¸Œëœë“œ ë§¤ì¹­: {brand} â†’ {data['default_product']}")
                    
                    detected_product = data['default_product']
                    for product in data["common_products"]:
                        if product.lower() in normalized_text:
                            detected_product = product
                            break
                    
                    return {
                        "ingredient": detected_product,
                        "brand": brand,
                        "category": category,
                        "confidence": 0.95,
                        "source": "brand_mapping",
                        "ocr_text": ocr_text
                    }
        
        for category, data in self.brand_mappings.items():
            for product in data["common_products"]:
                if product.lower() in normalized_text:
                    print(f"âœ… ì œí’ˆ ë§¤ì¹­: {product}")
                    return {
                        "ingredient": product,
                        "brand": "unknown",
                        "category": category,
                        "confidence": 0.85,
                        "source": "product_mapping",
                        "ocr_text": ocr_text
                    }
        
        print(f"âŒ ë¸Œëœë“œ/ì œí’ˆ ë§¤ì¹­ ì‹¤íŒ¨")
        return None
    def preprocess_image(self, image_input):
        """ì´ë¯¸ì§€ ê²½ë¡œ ë˜ëŠ” BytesIO ê°ì²´ ì²˜ë¦¬"""
        if isinstance(image_input, str) and os.path.exists(image_input):
            return cv2.imread(image_input)
        elif isinstance(image_input, BytesIO):
            image = PILImage.open(image_input).convert("RGB")
            return cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)
        else:
            print("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ì´ë¯¸ì§€ ì…ë ¥")
            return None
    # ===== ìƒ‰ìƒ ë° ëª¨ì–‘ ë¶„ì„ ê¸°ëŠ¥ =====
    
    def analyze_colors(self, image):
        """í–¥ìƒëœ ìƒ‰ìƒ ë¶„ì„ - ëª¨ë“  ì±„ì†Œ ìµœì í™”"""
        try:
            hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
            # ===== ğŸ†• ë°°ê²½ ë§ˆìŠ¤í‚¹ ì¶”ê°€ =====
            # ë„ˆë¬´ ë°ê±°ë‚˜ ì–´ë‘ìš´ ì˜ì—­ ì œì™¸ (ë°°ê²½ ì œê±°)
            _, mask = cv2.threshold(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), 30, 255, cv2.THRESH_BINARY)
            mask2 = cv2.threshold(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY), 240, 255, cv2.THRESH_BINARY_INV)[1]
            final_mask = cv2.bitwise_and(mask, mask2)
            
            # ë§ˆìŠ¤í¬ ì ìš©ëœ ì˜ì—­ë§Œ ìƒ‰ìƒ ë¶„ì„
            masked_hsv = cv2.bitwise_and(hsv, hsv, mask=final_mask)
            # ===== ğŸ¥— ì±„ì†Œë³„ ìµœì í™”ëœ ìƒ‰ìƒ ë²”ìœ„ =====
            color_ranges = {
                "ì–‘ë°°ì¶”ì´ˆë¡": [(25, 20, 40), (85, 150, 220)],    # ë§¤ìš° ë„“ì€ ì´ˆë¡ ë²”ìœ„
                "ì–‘ë°°ì¶”ë…¸ë‘": [(20, 30, 60), (45, 120, 200)],    # ì–‘ë°°ì¶” ì‹¬ ë¶€ë¶„
                
                # ë¹¨ê°„ìƒ‰ ê³„ì—´
                "ì§„í•œë¹¨ê°„ìƒ‰": [(0, 150, 100), (10, 255, 255)],      # í† ë§ˆí† , ë¹¨ê°„íŒŒí”„ë¦¬ì¹´
                "ì—°í•œë¹¨ê°„ìƒ‰": [(0, 80, 80), (10, 150, 200)],        # ë¶„í™ ë¬´, ì ìƒì¶”
                
                # ì£¼í™©ìƒ‰ ê³„ì—´  
                "ì§„í•œì£¼í™©ìƒ‰": [(11, 120, 100), (20, 255, 255)],     # ë‹¹ê·¼, ë‹¨í˜¸ë°•
                "ì—°í•œì£¼í™©ìƒ‰": [(11, 60, 80), (25, 150, 200)],       # ì—°í•œ ë‹¹ê·¼, ì˜¤ë Œì§€ íŒŒí”„ë¦¬ì¹´
                
                # ë…¸ë€ìƒ‰ ê³„ì—´
                "ì§„í•œë…¸ë€ìƒ‰": [(20, 120, 100), (30, 255, 255)],     # ë…¸ë€ íŒŒí”„ë¦¬ì¹´, ì˜¥ìˆ˜ìˆ˜
                "ì—°í•œë…¸ë€ìƒ‰": [(25, 40, 80), (35, 120, 200)],       # ì—°í•œ ì–‘ë°°ì¶”, ë°±ìƒ‰ ì–‘íŒŒ
                
                # ì´ˆë¡ìƒ‰ ê³„ì—´ (ê°€ì¥ ì„¸ë¶„í™”)
                "ì§„í•œì´ˆë¡ìƒ‰": [(40, 120, 80), (70, 255, 200)],      # ë¸Œë¡œì½œë¦¬, ì‹œê¸ˆì¹˜, ì¼€ì¼
                "ì¤‘ê°„ì´ˆë¡ìƒ‰": [(35, 80, 60), (75, 180, 180)],       # ì˜¤ì´, í”¼ë§, ì²­ê²½ì±„
                "ì—°í•œì´ˆë¡ìƒ‰": [(30, 30, 40), (85, 120, 160)],       # ì–‘ë°°ì¶”, ìƒì¶”, ë°°ì¶”
                "í™©ë¡ìƒ‰": [(25, 40, 60), (40, 120, 180)],           # ì—°í•œ ì–‘ë°°ì¶”, ì…€ëŸ¬ë¦¬
                
                # ë³´ë¼ìƒ‰ ê³„ì—´
                "ì§„í•œë³´ë¼ìƒ‰": [(120, 120, 80), (140, 255, 200)],    # ê°€ì§€, ììƒ‰ì–‘íŒŒ
                "ì—°í•œë³´ë¼ìƒ‰": [(110, 60, 60), (130, 150, 150)],     # ì—°í•œ ê°€ì§€, ë³´ë¼ì–‘ë°°ì¶”
                
                # ê°ˆìƒ‰/ë² ì´ì§€ ê³„ì—´
                "ì§„í•œê°ˆìƒ‰": [(8, 50, 30), (20, 180, 120)],          # ê°ì, ìƒê°•, ë§ˆëŠ˜ê»ì§ˆ
                "ì—°í•œê°ˆìƒ‰": [(10, 20, 40), (25, 80, 140)],          # ì–‘íŒŒ, ë§ˆëŠ˜, ì—°ê·¼
                "ë² ì´ì§€ìƒ‰": [(15, 15, 60), (30, 60, 160)],          # ë¬´, ì—°ê·¼, ì£½ìˆœ
                
                # í°ìƒ‰ ê³„ì—´
                "í°ìƒ‰": [(0, 0, 200), (180, 30, 255)],              # ë¬´, ì–‘íŒŒ, ë§ˆëŠ˜, ë°°ì¶”
                "íšŒìƒ‰": [(0, 0, 80), (180, 20, 180)]                # ë”ëŸ¬ìš´ ê°ì, ë•…ì´ ë¬»ì€ ì±„ì†Œ
            }
            # ë§ˆìŠ¤í¬ëœ ì˜ì—­ì—ì„œë§Œ ìƒ‰ìƒ ê³„ì‚°
            color_percentages = {}
            total_pixels = np.sum(final_mask > 0)  # ë§ˆìŠ¤í¬ëœ í”½ì…€ë§Œ ì¹´ìš´íŠ¸
            
            for color_name, (lower, upper) in color_ranges.items():
                lower = np.array(lower)
                upper = np.array(upper)
                
                color_mask = cv2.inRange(masked_hsv, lower, upper)
                color_pixels = np.sum(color_mask > 0)
                
                if total_pixels > 0:
                    percentage = (color_pixels / total_pixels) * 100
                    if percentage > 3:
                        color_percentages[color_name] = round(percentage, 1)

            # ìƒ‰ìƒ ê·¸ë£¹ë³„ í†µí•© ë¶„ì„
            grouped_colors = self._group_similar_colors(color_percentages)
            
            # ì£¼ìš” ìƒ‰ìƒ ìˆœì„œëŒ€ë¡œ ì •ë ¬
            dominant_colors = sorted(color_percentages.items(), key=lambda x: x[1], reverse=True)
            
            # ìµœì  ìƒ‰ìƒ ì„ íƒ (ì±„ì†Œ ì¸ì‹ ìš°ì„ )
            primary_color = self._select_primary_color_for_vegetables(grouped_colors, dominant_colors)
            
            return {
                "dominant_colors": dominant_colors[:3],
                "color_distribution": color_percentages,
                "grouped_colors": grouped_colors,
                "primary_color": primary_color
            }
            
        except Exception as e:
            print(f"âŒ ìƒ‰ìƒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"dominant_colors": [], "color_distribution": {}, "primary_color": "ì•Œìˆ˜ì—†ìŒ"}
        
    def _group_similar_colors(self, color_percentages):
        """ìœ ì‚¬ ìƒ‰ìƒ ê·¸ë£¹í™”"""
        groups = {
            "ë¹¨ê°„ê³„ì—´": ["ì§„í•œë¹¨ê°„ìƒ‰", "ì—°í•œë¹¨ê°„ìƒ‰"],
            "ì£¼í™©ê³„ì—´": ["ì§„í•œì£¼í™©ìƒ‰", "ì—°í•œì£¼í™©ìƒ‰"],
            "ë…¸ë€ê³„ì—´": ["ì§„í•œë…¸ë€ìƒ‰", "ì—°í•œë…¸ë€ìƒ‰"],
            "ì´ˆë¡ê³„ì—´": ["ì§„í•œì´ˆë¡ìƒ‰", "ì¤‘ê°„ì´ˆë¡ìƒ‰", "ì—°í•œì´ˆë¡ìƒ‰", "í™©ë¡ìƒ‰"],
            "ë³´ë¼ê³„ì—´": ["ì§„í•œë³´ë¼ìƒ‰", "ì—°í•œë³´ë¼ìƒ‰"],
            "ê°ˆìƒ‰ê³„ì—´": ["ì§„í•œê°ˆìƒ‰", "ì—°í•œê°ˆìƒ‰", "ë² ì´ì§€ìƒ‰"],
            "í°ìƒ‰ê³„ì—´": ["í°ìƒ‰", "íšŒìƒ‰"]
        }

        grouped = {}
        for group_name, colors in groups.items():
            total_percentage = sum(color_percentages.get(color, 0) for color in colors)
            if total_percentage > 5:
                grouped[group_name] = {
                    "total_percentage": total_percentage,
                    "dominant_color": max(colors, key=lambda c: color_percentages.get(c, 0))
                }

        return grouped
    def _select_primary_color_for_vegetables(self, grouped_colors, dominant_colors):
        """ì±„ì†Œ ì¸ì‹ì— ìµœì í™”ëœ ì£¼ìš” ìƒ‰ìƒ ì„ íƒ"""
        # ì´ˆë¡ê³„ì—´ì´ ìˆìœ¼ë©´ ìš°ì„  ì„ íƒ
        if "ì´ˆë¡ê³„ì—´" in grouped_colors:
            return grouped_colors["ì´ˆë¡ê³„ì—´"]["dominant_color"]
        
        # ê·¸ ì™¸ì—ëŠ” ê°€ì¥ dominantí•œ ìƒ‰ìƒ ì‚¬ìš©
        return dominant_colors[0][0] if dominant_colors else "ì•Œìˆ˜ì—†ìŒ"
    def analyze_shapes(self, image):
        """ëª¨ì–‘ ë¶„ì„"""
        try:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì ìš©
            blurred = cv2.GaussianBlur(gray, (5, 5), 0)
            
            # ì—£ì§€ ê²€ì¶œ
            edges = cv2.Canny(blurred, 50, 150)
            
            # ìœ¤ê³½ì„  ì°¾ê¸°
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            shapes_found = []
            
            for contour in contours:
                # ë©´ì ì´ ë„ˆë¬´ ì‘ì€ ìœ¤ê³½ì„  ì œì™¸
                area = cv2.contourArea(contour)
                if area < 1000:
                    continue
                
                # ìœ¤ê³½ì„  ê·¼ì‚¬í™”
                epsilon = 0.02 * cv2.arcLength(contour, True)
                approx = cv2.approxPolyDP(contour, epsilon, True)
                
                # ê²½ê³„ ìƒì
                x, y, w, h = cv2.boundingRect(contour)
                aspect_ratio = float(w) / h
                
                # ëª¨ì–‘ ë¶„ë¥˜
                shape_info = self._classify_shape(approx, aspect_ratio, area)
                shape_info.update({
                    "area": int(area),
                    "aspect_ratio": round(aspect_ratio, 2),
                    "bounding_box": [x, y, w, h]
                })
                
                shapes_found.append(shape_info)
            
            # ë©´ì  ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (í° ê²ƒë¶€í„°)
            shapes_found.sort(key=lambda x: x["area"], reverse=True)
            
            return {
                "total_objects": len(shapes_found),
                "shapes": shapes_found[:5],
                "primary_shape": shapes_found[0]["shape"] if shapes_found else "ì•Œìˆ˜ì—†ìŒ"
            }
            
        except Exception as e:
            print(f"âŒ ëª¨ì–‘ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"total_objects": 0, "shapes": [], "primary_shape": "ì•Œìˆ˜ì—†ìŒ"}

    def _classify_shape(self, approx, aspect_ratio, area):
        """ëª¨ì–‘ ë¶„ë¥˜"""
        vertices = len(approx)
        
        if vertices == 3:
            return {"shape": "ì‚¼ê°í˜•", "description": "ë¾°ì¡±í•œ í˜•íƒœ"}
        elif vertices == 4:
            if 0.95 <= aspect_ratio <= 1.05:
                return {"shape": "ì •ì‚¬ê°í˜•", "description": "ë„¤ëª¨ë‚œ í˜•íƒœ"}
            else:
                return {"shape": "ì§ì‚¬ê°í˜•", "description": "ê¸¸ì­‰í•œ í˜•íƒœ"}
        elif vertices > 8:
            # ì›í˜•ì„± ê³„ì‚°
            perimeter = cv2.arcLength(approx, True)
            circularity = 4 * np.pi * area / (perimeter * perimeter)
            
            if circularity > 0.7:
                if aspect_ratio > 1.5:
                    return {"shape": "íƒ€ì›í˜•", "description": "ê¸¸ì­‰í•œ ì›í˜• (ë°”ë‚˜ë‚˜í˜•)"}
                else:
                    return {"shape": "ì›í˜•", "description": "ë‘¥ê·¼ í˜•íƒœ (ì‚¬ê³¼, í† ë§ˆí† í˜•)"}
            else:
                return {"shape": "ë¶ˆê·œì¹™í˜•", "description": "ë³µì¡í•œ í˜•íƒœ"}
        else:
            return {"shape": f"{vertices}ê°í˜•", "description": "ë‹¤ê°í˜• í˜•íƒœ"}

    def analyze_sizes(self, image):
        """í¬ê¸° ë¶„ì„"""
        try:
            h, w = image.shape[:2]
            total_area = h * w
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # ì„ê³„ê°’ ì²˜ë¦¬ë¡œ ê°ì²´ ë¶„ë¦¬
            _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
            
            # ìœ¤ê³½ì„  ì°¾ê¸°
            contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            object_sizes = []
            
            for contour in contours:
                area = cv2.contourArea(contour)
                if area > 500:  # ìµœì†Œ í¬ê¸° í•„í„°
                    relative_size = (area / total_area) * 100
                    x, y, w, h = cv2.boundingRect(contour)
                    
                    size_category = "ì†Œí˜•"
                    if relative_size > 20:
                        size_category = "ëŒ€í˜•"
                    elif relative_size > 10:
                        size_category = "ì¤‘í˜•"
                    
                    object_sizes.append({
                        "absolute_area": int(area),
                        "relative_size": round(relative_size, 1),
                        "category": size_category,
                        "dimensions": [w, h]
                    })
            
            # í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬
            object_sizes.sort(key=lambda x: x["absolute_area"], reverse=True)
            
            return {
                "objects": object_sizes[:3],
                "size_distribution": self._categorize_sizes(object_sizes)
            }
            
        except Exception as e:
            print(f"âŒ í¬ê¸° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"objects": [], "size_distribution": {}}

    def predict_food_by_color(self, primary_color):
        """ìƒ‰ìƒ ê¸°ë°˜ ì‹í’ˆ ì˜ˆì¸¡ - ê¸°ì¡´ ê·¸ëŒ€ë¡œ"""
        color_food_map = {
            "ì§„í•œì£¼í™©ìƒ‰": ["ì˜¤ë Œì§€", "ë‹¹ê·¼", "ë‹¨í˜¸ë°•"],
            "ì—°í•œì£¼í™©ìƒ‰": ["ì˜¤ë Œì§€", "ê°", "ë³µìˆ­ì•„"],
            "ì£¼í™©ë¹¨ê°„ìƒ‰": ["ì˜¤ë Œì§€", "í† ë§ˆí† ", "ë¹¨ê°„íŒŒí”„ë¦¬ì¹´"],
            "ì§„í•œì´ˆë¡ìƒ‰": ["ë¸Œë¡œì½œë¦¬", "ì‹œê¸ˆì¹˜", "ì¼€ì¼"],
            "ì¤‘ê°„ì´ˆë¡ìƒ‰": ["ì˜¤ì´", "í”¼ë§", "ìƒì¶”"],
            "ì—°í•œì´ˆë¡ìƒ‰": ["ì–‘ë°°ì¶”", "ìƒì¶”", "ë°°ì¶”"],
            "ì§„í•œë¹¨ê°„ìƒ‰": ["í† ë§ˆí† ", "ë¹¨ê°„íŒŒí”„ë¦¬ì¹´", "ë”¸ê¸°"],
            "ì§„í•œë…¸ë€ìƒ‰": ["ë°”ë‚˜ë‚˜", "ë…¸ë€íŒŒí”„ë¦¬ì¹´", "ì˜¥ìˆ˜ìˆ˜"],
            "ì—°í•œë…¸ë€ìƒ‰": ["ë°”ë‚˜ë‚˜", "ì–‘íŒŒ", "ë ˆëª¬"],
            "ì§„í•œë³´ë¼ìƒ‰": ["ê°€ì§€", "ììƒ‰ì–‘íŒŒ", "í¬ë„"],
            "ì§„í•œê°ˆìƒ‰": ["ê°ì", "ìƒê°•", "ì–‘íŒŒ"],
            "í°ìƒ‰": ["ì–‘ë°°ì¶”", "ë¬´", "ë§ˆëŠ˜"]
        }
        
        return color_food_map.get(primary_color, ["ì‚¬ê³¼", "ë°”ë‚˜ë‚˜", "ë‹¹ê·¼"])
    
    def apply_general_corrections(self, detections, color_analysis):
        """ğŸ†• ì¼ë°˜í™”ëœ ë³´ì • ë¡œì§ ì ìš©"""
        primary_color = color_analysis.get("primary_color", "")
        
        for detection in detections:
            for rule in self.correction_rules:
                # ì¡°ê±´ í™•ì¸
                conditions = rule.get("conditions", {})
                
                # í´ë˜ìŠ¤ í™•ì¸
                if "detected_class" in conditions:
                    if detection['class'] not in conditions["detected_class"]:
                        continue
                
                # ìƒ‰ìƒ í™•ì¸
                if "color_match" in conditions:
                    if primary_color not in conditions["color_match"]:
                        continue
                
                # ì‹ ë¢°ë„ í™•ì¸
                if "confidence_threshold" in conditions:
                    if detection['confidence'] >= conditions["confidence_threshold"]:
                        continue
                
                # í˜•íƒœ ë¹„ìœ¨ í™•ì¸ (ì„ íƒì )
                if "shape_ratio" in conditions:
                    shape_ratio = conditions["shape_ratio"]
                    if "min_length_width_ratio" in shape_ratio:
                        ratio = detection['height'] / detection['width'] if detection['width'] > 0 else 0
                        if ratio < shape_ratio["min_length_width_ratio"]:
                            continue
                
                # ë³´ì • ì ìš©
                actions = rule.get("actions", {})
                print(f"ğŸ”§ ë³´ì • ì ìš©: {rule.get('name', 'unknown')} - {detection['class']} â†’ {actions.get('new_class', detection['class'])}")
                
                detection['original_class'] = detection['class']
                detection['original_korean_name'] = detection['korean_name']
                detection['class'] = actions.get('new_class', detection['class'])
                detection['korean_name'] = actions.get('new_korean_name', detection['korean_name'])
                
                # ì‹ ë¢°ë„ ì¡°ì •
                if 'confidence_boost' in actions:
                    new_confidence = detection['confidence'] + actions['confidence_boost']
                    max_confidence = actions.get('max_confidence', 1.0)
                    detection['confidence'] = min(new_confidence, max_confidence)
                
                detection['correction_applied'] = True
                detection['correction_reason'] = f"{rule.get('name', 'unknown')}: {rule.get('description', '')}"
                
                break  # ì²« ë²ˆì§¸ ë§¤ì¹­ ê·œì¹™ë§Œ ì ìš©
        
        return detections
    def analyze_shapes(self, image):
        """ëª¨ì–‘ ë¶„ì„"""
        try:
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # ê°€ìš°ì‹œì•ˆ ë¸”ëŸ¬ ì ìš©
            blurred = cv2.GaussianBlur(gray, (5, 5), 0)
            
            # ì—£ì§€ ê²€ì¶œ
            edges = cv2.Canny(blurred, 50, 150)
            
            # ìœ¤ê³½ì„  ì°¾ê¸°
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            shapes_found = []
            
            for contour in contours:
                # ë©´ì ì´ ë„ˆë¬´ ì‘ì€ ìœ¤ê³½ì„  ì œì™¸
                area = cv2.contourArea(contour)
                if area < 1000:
                    continue
                
                # ìœ¤ê³½ì„  ê·¼ì‚¬í™”
                epsilon = 0.02 * cv2.arcLength(contour, True)
                approx = cv2.approxPolyDP(contour, epsilon, True)
                
                # ê²½ê³„ ìƒì
                x, y, w, h = cv2.boundingRect(contour)
                aspect_ratio = float(w) / h
                
                # ëª¨ì–‘ ë¶„ë¥˜
                shape_info = self._classify_shape(approx, aspect_ratio, area)
                shape_info.update({
                    "area": int(area),
                    "aspect_ratio": round(aspect_ratio, 2),
                    "bounding_box": [x, y, w, h]
                })
                
                shapes_found.append(shape_info)
            
            # ë©´ì  ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (í° ê²ƒë¶€í„°)
            shapes_found.sort(key=lambda x: x["area"], reverse=True)
            
            return {
                "total_objects": len(shapes_found),
                "shapes": shapes_found[:5],
                "primary_shape": shapes_found[0]["shape"] if shapes_found else "ì•Œìˆ˜ì—†ìŒ"
            }
            
        except Exception as e:
            print(f"âŒ ëª¨ì–‘ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"total_objects": 0, "shapes": [], "primary_shape": "ì•Œìˆ˜ì—†ìŒ"}
    def _predict_food_by_properties(self, color, shape, size_info):
        """ìƒ‰ìƒê³¼ ëª¨ì–‘ìœ¼ë¡œ ì±„ì†Œ ì˜ˆì¸¡ - ì „ì²´ ì±„ì†Œ ìµœì í™”"""
        predictions = []
        
        # ===== ğŸ¥— ì±„ì†Œë³„ ìƒì„¸ ì˜ˆì¸¡ ê·œì¹™ =====
        
        # 1. ìƒ‰ìƒ + ëª¨ì–‘ ì¡°í•© ìš°ì„  ê·œì¹™
        if color == "í°ìƒ‰":
            # í°ìƒ‰ìœ¼ë¡œ ì˜ëª» ì¸ì‹ëœ ê²½ìš° ì–‘ë°°ì¶” ìš°ì„ 
            predictions = ["ì–‘ë°°ì¶”", "ë°°ì¶”", "ìƒì¶”", "ë¬´", "ì–‘íŒŒ"]
            print(f"ğŸ¥¬ í°ìƒ‰ ê°ì§€ â†’ ì–‘ë°°ì¶” ìš°ì„  ì˜ˆì¸¡: {predictions}")
            return predictions
        
        detailed_rules = {
            # ë¹¨ê°„ìƒ‰ ê³„ì—´
            ("ì§„í•œë¹¨ê°„ìƒ‰", "ì›í˜•"): ["í† ë§ˆí† ", "ë¹¨ê°„íŒŒí”„ë¦¬ì¹´"],
            ("ì§„í•œë¹¨ê°„ìƒ‰", "ë¶ˆê·œì¹™í˜•"): ["ë¹¨ê°„íŒŒí”„ë¦¬ì¹´", "í† ë§ˆí† "],
            ("ì—°í•œë¹¨ê°„ìƒ‰", "ì§ì‚¬ê°í˜•"): ["ì ìƒì¶”", "ì ì–‘ë°°ì¶”"],
            
            # ì£¼í™©ìƒ‰ ê³„ì—´
            ("ì§„í•œì£¼í™©ìƒ‰", "ì›í˜•"): ["ë‹¨í˜¸ë°•", "ë‹¹ê·¼"],
            ("ì§„í•œì£¼í™©ìƒ‰", "ì§ì‚¬ê°í˜•"): ["ë‹¹ê·¼"],
            ("ì§„í•œì£¼í™©ìƒ‰", "íƒ€ì›í˜•"): ["ë‹¹ê·¼", "ê³ êµ¬ë§ˆ"],
            ("ì—°í•œì£¼í™©ìƒ‰", "ë¶ˆê·œì¹™í˜•"): ["í˜¸ë°•", "ë‹¹ê·¼"],
            
            # ë…¸ë€ìƒ‰ ê³„ì—´
            ("ì§„í•œë…¸ë€ìƒ‰", "ë¶ˆê·œì¹™í˜•"): ["ë…¸ë€íŒŒí”„ë¦¬ì¹´", "ì˜¥ìˆ˜ìˆ˜"],
            ("ì—°í•œë…¸ë€ìƒ‰", "ë¶ˆê·œì¹™í˜•"): ["ì–‘ë°°ì¶”", "í°ì–‘íŒŒ"],
            ("ì—°í•œë…¸ë€ìƒ‰", "ì›í˜•"): ["ì–‘íŒŒ", "ê°ì"],
            
            # ì´ˆë¡ìƒ‰ ê³„ì—´ (ê°€ì¥ ì¤‘ìš”)
            ("ì§„í•œì´ˆë¡ìƒ‰", "ë¶ˆê·œì¹™í˜•"): ["ë¸Œë¡œì½œë¦¬", "ì‹œê¸ˆì¹˜", "ì¼€ì¼"],
            ("ì§„í•œì´ˆë¡ìƒ‰", "ì›í˜•"): ["ë¸Œë¡œì½œë¦¬", "ì–‘ë°°ì¶”ì‹¬"],
            ("ì¤‘ê°„ì´ˆë¡ìƒ‰", "ì§ì‚¬ê°í˜•"): ["ì˜¤ì´", "í”¼ë§"],
            ("ì¤‘ê°„ì´ˆë¡ìƒ‰", "ë¶ˆê·œì¹™í˜•"): ["í”¼ë§", "ì²­ê²½ì±„", "ìƒì¶”"],
            ("ì—°í•œì´ˆë¡ìƒ‰", "ë¶ˆê·œì¹™í˜•"): ["ì–‘ë°°ì¶”", "ìƒì¶”", "ë°°ì¶”", "ì²­ê²½ì±„"],
            ("ì—°í•œì´ˆë¡ìƒ‰", "ì›í˜•"): ["ì–‘ë°°ì¶”", "ìƒì¶”"],
            ("í™©ë¡ìƒ‰", "ë¶ˆê·œì¹™í˜•"): ["ì—°í•œì–‘ë°°ì¶”", "ì…€ëŸ¬ë¦¬", "ë°°ì¶”"],
            
            # ë³´ë¼ìƒ‰ ê³„ì—´
            ("ì§„í•œë³´ë¼ìƒ‰", "íƒ€ì›í˜•"): ["ê°€ì§€"],
            ("ì§„í•œë³´ë¼ìƒ‰", "ì›í˜•"): ["ììƒ‰ì–‘íŒŒ", "ê°€ì§€"],
            ("ì—°í•œë³´ë¼ìƒ‰", "ë¶ˆê·œì¹™í˜•"): ["ë³´ë¼ì–‘ë°°ì¶”", "ì ì–‘íŒŒ"],
            
            # ê°ˆìƒ‰ ê³„ì—´
            ("ì§„í•œê°ˆìƒ‰", "ë¶ˆê·œì¹™í˜•"): ["ê°ì", "ìƒê°•", "ë”ë•"],
            ("ì§„í•œê°ˆìƒ‰", "ì›í˜•"): ["ê°ì", "ì–‘íŒŒ"],
            ("ì—°í•œê°ˆìƒ‰", "ì›í˜•"): ["ì–‘íŒŒ", "ë§ˆëŠ˜", "ê°ì"],
            ("ë² ì´ì§€ìƒ‰", "ì§ì‚¬ê°í˜•"): ["ë¬´", "ì—°ê·¼"],
            ("ë² ì´ì§€ìƒ‰", "ë¶ˆê·œì¹™í˜•"): ["ë¬´", "ë„ë¼ì§€", "ë”ë•"],
            
            # í°ìƒ‰ ê³„ì—´
            ("í°ìƒ‰", "ì§ì‚¬ê°í˜•"): ["ë¬´", "ëŒ€íŒŒ"],
            ("í°ìƒ‰", "ë¶ˆê·œì¹™í˜•"): ["ë¬´", "ì–‘íŒŒ", "ë§ˆëŠ˜", "ë°°ì¶”"],
            ("í°ìƒ‰", "ì›í˜•"): ["ì–‘íŒŒ", "ë§ˆëŠ˜", "ë¬´"]
        }
        
        # ì •í™•í•œ ë§¤ì¹­ ì‹œë„
        key = (color, shape)
        if key in detailed_rules:
            predictions.extend(detailed_rules[key])
            print(f"ğŸ¯ ì •í™• ë§¤ì¹­: {color} + {shape} â†’ {detailed_rules[key]}")
            return predictions
        
        # 2. ìƒ‰ìƒë§Œìœ¼ë¡œ ì˜ˆì¸¡ (ëª¨ì–‘ì´ ë¶ˆë¶„ëª…í•œ ê²½ìš°)
        if not predictions and shape in ["ì•Œìˆ˜ì—†ìŒ", "ë¶ˆê·œì¹™í˜•"]:
            color_priority_rules = {
                # ì´ˆë¡ìƒ‰ ê³„ì—´ (ì„¸ë¶„í™”)
                "ì§„í•œì´ˆë¡ìƒ‰": ["ë¸Œë¡œì½œë¦¬", "ì‹œê¸ˆì¹˜", "ì¼€ì¼", "í”¼ë§"],
                "ì¤‘ê°„ì´ˆë¡ìƒ‰": ["ì˜¤ì´", "í”¼ë§", "ìƒì¶”", "ì²­ê²½ì±„"],
                "ì—°í•œì´ˆë¡ìƒ‰": ["ì–‘ë°°ì¶”", "ìƒì¶”", "ë°°ì¶”", "ì²­ê²½ì±„"],
                "í™©ë¡ìƒ‰": ["ì–‘ë°°ì¶”", "ì…€ëŸ¬ë¦¬", "ë°°ì¶”", "íŒŒ"],
                
                # ë¹¨ê°„ìƒ‰ ê³„ì—´
                "ì§„í•œë¹¨ê°„ìƒ‰": ["í† ë§ˆí† ", "ë¹¨ê°„íŒŒí”„ë¦¬ì¹´", "ê³ ì¶”"],
                "ì—°í•œë¹¨ê°„ìƒ‰": ["ì ìƒì¶”", "ì ì–‘ë°°ì¶”", "ì–‘íŒŒ"],
                
                # ì£¼í™©ìƒ‰ ê³„ì—´
                "ì§„í•œì£¼í™©ìƒ‰": ["ë‹¹ê·¼", "ë‹¨í˜¸ë°•", "ê³ êµ¬ë§ˆ"],
                "ì—°í•œì£¼í™©ìƒ‰": ["ë‹¹ê·¼", "í˜¸ë°•", "ë‹¨í˜¸ë°•"],
                
                # ë…¸ë€ìƒ‰ ê³„ì—´
                "ì§„í•œë…¸ë€ìƒ‰": ["ë…¸ë€íŒŒí”„ë¦¬ì¹´", "ì˜¥ìˆ˜ìˆ˜", "ë‹¨í˜¸ë°•"],
                "ì—°í•œë…¸ë€ìƒ‰": ["ì–‘ë°°ì¶”", "ì–‘íŒŒ", "ê°ì"],
                
                # ë³´ë¼ìƒ‰ ê³„ì—´
                "ì§„í•œë³´ë¼ìƒ‰": ["ê°€ì§€", "ììƒ‰ì–‘íŒŒ", "ë³´ë¼ì–‘ë°°ì¶”"],
                "ì—°í•œë³´ë¼ìƒ‰": ["ë³´ë¼ì–‘ë°°ì¶”", "ì ì–‘íŒŒ"],
                
                # ê°ˆìƒ‰ ê³„ì—´
                "ì§„í•œê°ˆìƒ‰": ["ê°ì", "ìƒê°•", "ìš°ì—‰", "ë”ë•"],
                "ì—°í•œê°ˆìƒ‰": ["ì–‘íŒŒ", "ë§ˆëŠ˜", "ê°ì", "ì—°ê·¼"],
                "ë² ì´ì§€ìƒ‰": ["ë¬´", "ì—°ê·¼", "ë„ë¼ì§€", "ì£½ìˆœ"],
                
                # í°ìƒ‰ ê³„ì—´
                "í°ìƒ‰": ["ì–‘ë°°ì¶”", "ë¬´", "ì–‘íŒŒ", "ë§ˆëŠ˜", "ë°°ì¶”", "ëŒ€íŒŒ"],
                "íšŒìƒ‰": ["ê°ì", "ë”ë•", "ìš°ì—‰"]
            }
            
            if color in color_priority_rules:
                predictions.extend(color_priority_rules[color])
                print(f"ğŸ¨ ìƒ‰ìƒ ìš°ì„  ì˜ˆì¸¡: {color} â†’ {color_priority_rules[color]}")
        
        # 3. í¬ê¸° ì •ë³´ ë³´ì • (ìˆëŠ” ê²½ìš°)
        if predictions and size_info:
            predictions = self._adjust_predictions_by_size(predictions, size_info)
        
        # 4. ìµœì¢… ì˜ˆì¸¡ì´ ì—†ìœ¼ë©´ ì¼ë°˜ì ì¸ ì±„ì†Œë“¤
        if not predictions:
            predictions = ["ì–‘ë°°ì¶”", "ìƒì¶”", "ë¬´", "ë‹¹ê·¼", "ê°ì"]
            print(f"â“ ê¸°ë³¸ ì±„ì†Œ ì˜ˆì¸¡: {predictions}")
        
        return predictions[:3]  # ìƒìœ„ 3ê°œë§Œ ë°˜í™˜
    def _adjust_predictions_by_size(self, predictions, size_info):
        """í¬ê¸° ì •ë³´ë¡œ ì˜ˆì¸¡ ë³´ì •"""
        try:
            if not size_info or not isinstance(size_info, dict):
                return predictions
            
            # í¬ê¸°ë³„ ì±„ì†Œ ë¶„ë¥˜
            large_vegetables = ["ì–‘ë°°ì¶”", "ë°°ì¶”", "ë¬´", "ë¸Œë¡œì½œë¦¬", "ë‹¨í˜¸ë°•"]
            medium_vegetables = ["í† ë§ˆí† ", "ê°€ì§€", "í”¼ë§", "íŒŒí”„ë¦¬ì¹´", "ì˜¤ì´"]
            small_vegetables = ["ë§ˆëŠ˜", "ìƒê°•", "ê³ ì¶”", "ì²´ë¦¬í† ë§ˆí† "]
            
            size_category = size_info.get("category", "")
            
            if size_category == "ëŒ€í˜•":
                # í° ì±„ì†Œ ìš°ì„ 
                reordered = [v for v in predictions if v in large_vegetables]
                reordered.extend([v for v in predictions if v not in large_vegetables])
                return reordered
            elif size_category == "ì†Œí˜•":
                # ì‘ì€ ì±„ì†Œ ìš°ì„ 
                reordered = [v for v in predictions if v in small_vegetables]
                reordered.extend([v for v in predictions if v not in small_vegetables])
                return reordered
            
            return predictions
            
        except Exception as e:
            print(f"âš ï¸ í¬ê¸° ë³´ì • ì‹¤íŒ¨: {e}")
            return predictions    
    def _categorize_sizes(self, object_sizes):
        """í¬ê¸° ë¶„í¬ ë¶„ì„"""
        categories = {"ëŒ€í˜•": 0, "ì¤‘í˜•": 0, "ì†Œí˜•": 0}
        
        for obj in object_sizes:
            category = obj["category"]
            categories[category] += 1
        
        return categories

    def analyze_image_properties(self, image):
        """ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ, ëª¨ì–‘, í¬ê¸° ì¢…í•© ë¶„ì„"""
        try:
            # ìƒ‰ìƒ ë¶„ì„
            color_analysis = self.analyze_colors(image)
            
            # ëª¨ì–‘ ë¶„ì„
            shape_analysis = self.analyze_shapes(image)
            
            # í¬ê¸° ë¶„ì„
            size_analysis = self.analyze_sizes(image)
            
            # ì „ì²´ì ì¸ í‰ê°€ ìƒì„±
            overall_assessment = self._create_overall_assessment(
                color_analysis, shape_analysis, size_analysis
            )
            
            return {
                "colors": color_analysis,
                "shapes": shape_analysis,
                "sizes": size_analysis,
                "overall_assessment": overall_assessment
            }
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ì†ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None

    def _create_overall_assessment(self, color_analysis, shape_analysis, size_analysis):
        """ì „ì²´ì ì¸ í‰ê°€ ìƒì„±"""
        assessment = {
            "likely_food_type": "ì•Œìˆ˜ì—†ìŒ",
            "confidence_indicators": [],
            "analysis_summary": ""
        }
        
        try:
            # ìƒ‰ìƒ + ëª¨ì–‘ ì¡°í•©ìœ¼ë¡œ ìŒì‹ ì¶”ì •
            primary_color = color_analysis.get("primary_color", "")
            primary_shape = shape_analysis.get("primary_shape", "")
            
            # ìŒì‹ ì¶”ì • ë¡œì§
            food_predictions = self._predict_food_by_properties(primary_color, primary_shape, size_analysis)
            
            if food_predictions:
                assessment["likely_food_type"] = food_predictions[0]
                assessment["confidence_indicators"] = [
                    f"ì£¼ìš” ìƒ‰ìƒ: {primary_color}",
                    f"ì£¼ìš” í˜•íƒœ: {primary_shape}",
                    f"ê°ì²´ ìˆ˜: {shape_analysis.get('total_objects', 0)}"
                ]
            
            # ìš”ì•½ ìƒì„±
            summary_parts = []
            if color_analysis.get("dominant_colors"):
                colors = [color for color, _ in color_analysis["dominant_colors"]]
                summary_parts.append(f"ì£¼ìš” ìƒ‰ìƒ: {', '.join(colors)}")
            
            if primary_shape != "ì•Œìˆ˜ì—†ìŒ":
                summary_parts.append(f"í˜•íƒœ: {primary_shape}")
            
            assessment["analysis_summary"] = " | ".join(summary_parts)
            
        except Exception as e:
            print(f"âš ï¸ ì „ì²´ í‰ê°€ ìƒì„± ì‹¤íŒ¨: {e}")
        
        return assessment
    
    def enhanced_vegetable_analysis(self, image_input, confidence=0.5):
        """ì±„ì†Œ ì „ìš© í–¥ìƒ ë¶„ì„"""
        try:
            image = self.preprocess_image(image_input)
            if image is None:
                return {"success": False, "error": "ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨"}
            
            print("ğŸ¥— ì±„ì†Œ ì „ìš© ë¶„ì„ ì‹œì‘...")
            
            # 1. ê¸°ë³¸ YOLO íƒì§€
            yolo_detections, _ = self.detect_objects(image, confidence)
            
            # 2. í–¥ìƒëœ ìƒ‰ìƒ ë¶„ì„
            color_analysis = self.analyze_colors(image)
            
            # 3. ëª¨ì–‘ ë¶„ì„
            shape_analysis = self.analyze_shapes(image)
            
            # 4. ì±„ì†Œ íŠ¹í™” ì˜ˆì¸¡
            primary_color = color_analysis.get("primary_color", "")
            primary_shape = shape_analysis.get("primary_shape", "")
            
            vegetable_predictions = self._predict_food_by_properties(
                primary_color, primary_shape, shape_analysis.get("sizes", {})
            )
            print(f"âœ… ì±„ì†Œ ë¶„ì„ ì™„ë£Œ: {vegetable_predictions}")

            # 5. ì‹ ë¢°ë„ ê³„ì‚°
            confidence_score = self._create_vegetable_confidence_score(
                vegetable_predictions, color_analysis, shape_analysis
            )
            
            # 6. ê²°ê³¼ í†µí•©
            enhanced_detections = []
            
            for detection in yolo_detections:
                if detection.get("class") in ["item", "unknown", "object"]:
                    # ì±„ì†Œ ì˜ˆì¸¡ìœ¼ë¡œ êµì²´
                    if vegetable_predictions:
                        detection["class"] = vegetable_predictions[0]
                        detection["originalClass"] = "item"
                        detection["confidence"] = confidence_score
                        detection["source"] = "vegetable_enhanced"
                        detection["predictions"] = vegetable_predictions[:3]
                        detection["color_info"] = primary_color
                        detection["shape_info"] = primary_shape
                
                enhanced_detections.append(detection)
            
            result = {
                "success": True,
                "detections": enhanced_detections,
                "vegetable_analysis": {
                    "predicted_vegetables": vegetable_predictions,
                    "confidence": confidence_score,
                    "color_analysis": color_analysis,
                    "shape_analysis": shape_analysis,
                    "analysis_method": "vegetable_specialized"
                }
            }
            
            print(f"âœ… ì±„ì†Œ ë¶„ì„ ì™„ë£Œ: {vegetable_predictions}")
            return result
            
        except Exception as e:
            print(f"âŒ ì±„ì†Œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": f"ì±„ì†Œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}"}    
        # ===== GPT í†µí•© ë¶„ì„ ê¸°ëŠ¥ =====
    def ensemble_detection(self, image_path, confidence=0.5):
        """ì•™ìƒë¸” íƒì§€ + ìƒ‰ìƒ ë¶„ì„ + ì¼ë°˜í™”ëœ ë³´ì •"""
        try:
            print(f"ğŸ” ì•™ìƒë¸” íƒì§€ ì‹œì‘...")
            
            all_detections = []
            
            # 1. ê° ëª¨ë¸ë¡œ íƒì§€
            for model_name, model in self.models.items():
                try:
                    detections, _ = detect_objects(model, image_path, confidence)
                    
                    for det in detections:
                        det['model_source'] = model_name
                        det['enhanced'] = False
                        det['correction_applied'] = False
                    
                    all_detections.extend(detections)
                    print(f"   âœ… {model_name}: {len(detections)}ê°œ íƒì§€")
                    
                except Exception as e:
                    print(f"   âŒ {model_name} íƒì§€ ì‹¤íŒ¨: {e}")
            
            # 2. ì¤‘ë³µ ì œê±°
            merged_detections = merge_similar_detections(all_detections)
            
            # 3. ìƒ‰ìƒ ë¶„ì„
            image = cv2.imread(image_path)
            color_analysis = None
            # 25.06.08 ì¶”ê°€ 
            shape_analysis = None
            size_analysis = None
            if image is not None:
                color_analysis = self.analyze_colors(image)
                shape_analysis = self.analyze_shapes(image)
                size_analysis = self.analyze_sizes(image)
                primary_color = color_analysis["primary_color"]
                
                # ğŸ†• ì¼ë°˜í™”ëœ ë³´ì • ì ìš©
                merged_detections = self.apply_general_corrections(merged_detections, color_analysis)
                
                # ë‚®ì€ ì‹ ë¢°ë„ ê²°ê³¼ë¥¼ ìƒ‰ìƒ ê¸°ë°˜ìœ¼ë¡œ ë³´ì™„
                food_predictions = self.predict_food_by_color(primary_color)
                  # ğŸ§© ëª¨ë“  íƒì§€ ê²°ê³¼ì— ë¶„ì„ ê²°ê³¼ ë³‘í•©
                for detection in merged_detections:
                    if detection['confidence'] < 0.6 and not detection.get('correction_applied', False):
                        if food_predictions:
                            detection["color_info"] = color_analysis.get("primary_color", "ì•Œìˆ˜ì—†ìŒ")
                            detection["shape_info"] = shape_analysis.get("primary_shape", "ì•Œìˆ˜ì—†ìŒ")
                            detection["size_info"] = size_analysis.get("objects", [])
                            # ìƒ‰ìƒ ê¸°ë°˜ ì˜ˆì¸¡ë„ ë³´ì™„
                            if detection['confidence'] < 0.6 and not detection.get('correction_applied', False):
                                food_predictions = self.predict_food_by_color(primary_color)
                            if food_predictions:
                                detection['enhanced_prediction'] = food_predictions[0]
                                detection['enhanced_alternatives'] = food_predictions[1:3]
                                detection['enhanced'] = True
                            print(f"   ğŸ¨ ìƒ‰ìƒ ê¸°ë°˜ ë³´ì™„: {detection['class']} â†’ {food_predictions[0]}")
            
            return {
                "detections": merged_detections,
                "color_analysis": color_analysis,
                "total_enhanced": sum(1 for det in merged_detections if det.get('enhanced', False)),
                "total_corrected": sum(1 for det in merged_detections if det.get('correction_applied', False))
            }
            
        except Exception as e:
            print(f"âŒ ì•™ìƒë¸” íƒì§€ ì‹¤íŒ¨: {e}")
            return {"detections": [], "color_analysis": None, "total_enhanced": 0, "total_corrected": 0}
    
    def comprehensive_analysis(self, image_path, confidence=0.5, ocr_text=None):
        """3ë‹¨ê³„ ì™„ì „ í†µí•© ë¶„ì„ - ê¸°ì¡´ API ìœ ì§€"""
        try:
            print(f"ğŸ”¬ ì™„ì „ í†µí•© ë¶„ì„ ì‹œì‘: {image_path}")
            
            # 1ë‹¨ê³„: OCR ìš°ì„  ë¸Œëœë“œ ì¸ì‹
            if ocr_text:
                print(f"ğŸ“„ 1ë‹¨ê³„: OCR ë¸Œëœë“œ ì¸ì‹ ì‹œë„")
                ocr_result = self.analyze_ocr_first(ocr_text)
                if ocr_result and ocr_result['confidence'] >= 0.85:
                    print(f"âœ… OCR ë¸Œëœë“œ ì¸ì‹ ì„±ê³µ: {ocr_result['ingredient']}")
                    return {
                        "success": True,
                        "detections": [{
                            'id': 0,
                            'class': ocr_result['ingredient'],
                            'korean_name': ocr_result['ingredient'],
                            'confidence': ocr_result['confidence'],
                            'source': ocr_result['source'],
                            'brand': ocr_result.get('brand', 'unknown'),
                            'category': ocr_result.get('category', 'unknown'),
                            'bbox': [0, 0, 100, 100],
                            'method': 'ocr_brand_mapping'
                        }],
                        "method": "ocr_priority",
                        "analysis_stage": "1_ocr_brand_recognition",
                        "confidence": ocr_result["confidence"],
                        "brand_info": {
                            "brand": ocr_result.get('brand'),
                            "category": ocr_result.get('category')
                        }
                    }
            
            # 2ë‹¨ê³„: ì•™ìƒë¸” íƒì§€ + ìƒ‰ìƒ ë¶„ì„ + ë³´ì •
            print(f"ğŸ¯ 2ë‹¨ê³„: ì•™ìƒë¸” + ìƒ‰ìƒ ë¶„ì„ + ë³´ì •")
            ensemble_result = self.ensemble_detection(image_path, confidence)
             
            all_detections = ensemble_result["detections"]
            food_detections = filter_food_detections(all_detections)
            # 25.06.08 ì¶”ê°€ 
            image = cv2.imread(image_path)

            # 3ë‹¨ê³„: ì±„ì†Œ íŠ¹í™” ë¶„ì„ (ë³´ì™„ìš©)
            vegetable_result = None
            if len(food_detections) == 0 and image is not None:
                print("ğŸ¥¦ íƒì§€ ê²°ê³¼ ë¶€ì¡± â†’ ì±„ì†Œ ì „ìš© ë¶„ì„ ì‹œë„")
                vegetable_result = self.enhanced_vegetable_analysis(image)

                if vegetable_result.get("success") and vegetable_result["vegetable_analysis"]["predicted_vegetables"]:
                    predicted = vegetable_result["vegetable_analysis"]["predicted_vegetables"]
                    primary_color = vegetable_result["vegetable_analysis"]["color_analysis"]["primary_color"]
                    primary_shape = vegetable_result["vegetable_analysis"]["shape_analysis"]["primary_shape"]

                    # ê¸°ì¡´ íƒì§€ ê²°ê³¼ ë³´ì • ì‹œë„
                    for det in all_detections:
                        if det["class"] in ["item", "apple", "object", "unknown"]:
                            print(f"ğŸ” '{det['class']}' â†’ '{predicted[0]}' ë³´ì •")
                            det["original_class"] = det["class"]
                            det["class"] = predicted[0]
                            det["confidence"] = vegetable_result["vegetable_analysis"]["confidence"]
                            det["color_info"] = primary_color
                            det["shape_info"] = primary_shape
                            det["source"] = "vegetable_inference"
                            det["method"] = "vegetable_repair"
                            det["enhanced"] = True
   
                    food_detections = filter_food_detections(all_detections)
         
            print(f"ğŸ 3ë‹¨ê³„: ìŒì‹ í•„í„°ë§")    
            # 4ë‹¨ê³„: ê²°ê³¼ ë¶„ì„
            analysis = analyze_detection_results(food_detections)
            
            # 5ë‹¨ê³„: ìµœì¢… ê²°ê³¼ êµ¬ì„±
            result = {
                "success": True,
                "detections": food_detections,
                "color_analysis": ensemble_result["color_analysis"],
                "statistics": analysis,
                "method": "comprehensive_enhanced",
                "analysis_stage": "2_ensemble_color_correction",
                "enhancement_info": {
                    "total_detections": len(all_detections),
                    "food_filtered": len(food_detections),
                    "enhanced_predictions": ensemble_result["total_enhanced"],
                    "corrections_applied": ensemble_result["total_corrected"],
                    "models_used": list(self.models.keys()),
                    "vegetable_inference_used": vegetable_result is not None
                }
            }
            if vegetable_result:
                result["vegetable_analysis"] = vegetable_result["vegetable_analysis"]
            
            print(f"âœ… ì™„ì „ í†µí•© ë¶„ì„ ì™„ë£Œ:")
            print(f"   ğŸ“Š ì´ {len(food_detections)}ê°œ ìŒì‹ ê°ì²´")
            print(f"   ğŸ¨ {ensemble_result['total_enhanced']}ê°œ ìƒ‰ìƒ ë³´ì™„")
            print(f"   ğŸ”§ {ensemble_result['total_corrected']}ê°œ ë³´ì • ì ìš©")
            
            return result
            
        except Exception as e:
            print(f"âŒ ì™„ì „ í†µí•© ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "method": "error"}

# ===== í¸ì˜ í•¨ìˆ˜ë“¤ (ê¸°ì¡´ API ìœ ì§€) =====

def create_enhanced_detector(model_paths=None):
    """í–¥ìƒëœ íƒì§€ê¸° ìƒì„± - ê¸°ì¡´ API ìœ ì§€"""
    if model_paths is None:
        model_paths = {
            'yolo11s': 'models/yolo11s.pt',
            'best': 'models/best.pt',
            'best_friged': 'models/best_friged.pt'
        }
    
    models = {}
    for name, path in model_paths.items():
        model = get_cached_model(path)
        if model:
            models[name] = model
            print(f"âœ… {name} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        else:
            print(f"âŒ {name} ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
    
    detector = EnhancedYOLODetector(models)
    print(f"ğŸš€ Enhanced YOLO íƒì§€ê¸° ìƒì„± ì™„ë£Œ:")
    print(f"   ğŸ“¦ ëª¨ë¸: {len(models)}ê°œ")
    print(f"   ğŸ·ï¸ ë¸Œëœë“œ ì¹´í…Œê³ ë¦¬: {len(BRAND_MAPPINGS)}ê°œ")
    print(f"   ğŸ”§ ë³´ì • ê·œì¹™: {len(CORRECTION_RULES)}ê°œ")
    
    return detector

def quick_food_detection(image_path, confidence=0.5, use_enhanced=True, ocr_text=None):
    """ë¹ ë¥¸ ìŒì‹ íƒì§€ (ì˜¬ì¸ì› í•¨ìˆ˜) - ê¸°ì¡´ API ìœ ì§€"""
    try:
        if use_enhanced:
            # í–¥ìƒëœ íƒì§€ê¸° ì‚¬ìš©
            detector = create_enhanced_detector()
            result = detector.comprehensive_analysis(image_path, confidence, ocr_text)
            return result
        else:
            # ê¸°ë³¸ ë‹¨ì¼ ëª¨ë¸ ì‚¬ìš©
            model = get_cached_model()
            if not model:
                return {"success": False, "error": "ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨"}
            
            detections, _ = detect_objects(model, image_path, confidence)
            food_detections = filter_food_detections(detections)
            analysis = analyze_detection_results(food_detections)
            
            return {
                "success": True,
                "detections": food_detections,
                "statistics": analysis,
                "method": "basic_yolo"
            }
            
    except Exception as e:
        return {"success": False, "error": str(e)}

# ===== í…ŒìŠ¤íŠ¸ ë° ìœ í‹¸ë¦¬í‹° (ê¸°ì¡´ ìœ ì§€) =====

def test_enhanced_detector(test_image_path=None, test_ocr_text=None):
    """Enhanced YOLO íƒì§€ê¸° í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Enhanced YOLO íƒì§€ê¸° ì¢…í•© í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # Enhanced íƒì§€ê¸° ìƒì„±
    detector = create_enhanced_detector()
    
    if not detector.models:
        print("âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ - í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
        return None
    
    # ë¸Œëœë“œ ì¸ì‹ í…ŒìŠ¤íŠ¸
    print("\nğŸ“‹ 1. ë¸Œëœë“œ ì¸ì‹ í…ŒìŠ¤íŠ¸")
    test_brands = [
        "ë©”ê°€ì»¤í”¼ ì•„ë©”ë¦¬ì¹´ë…¸",
        "ìŠ¤íƒ€ë²…ìŠ¤ ì¹´í˜ë¼ë–¼", 
        "ë¡¯ë° ì´ˆì½”íŒŒì´",
        "ë§¤ì¼ìœ ì—… ìš°ìœ "
    ]
    
    for brand_text in test_brands:
        result = detector.analyze_ocr_first(brand_text)
        if result:
            print(f"   âœ… '{brand_text}' â†’ {result['ingredient']} ({result['confidence']:.2f})")
        else:
            print(f"   âŒ '{brand_text}' â†’ ì¸ì‹ ì‹¤íŒ¨")
    
    # ì´ë¯¸ì§€ ë¶„ì„ í…ŒìŠ¤íŠ¸
    if test_image_path and os.path.exists(test_image_path):
        print(f"\nğŸ–¼ï¸ 2. ì´ë¯¸ì§€ ë¶„ì„ í…ŒìŠ¤íŠ¸: {test_image_path}")
        
        # ê¸°ë³¸ íƒì§€
        basic_result = quick_food_detection(test_image_path, use_enhanced=False)
        print(f"ğŸ“Š ê¸°ë³¸ íƒì§€: {basic_result.get('statistics', {}).get('total_objects', 0)}ê°œ")
        
        # Enhanced íƒì§€ (OCR ì—†ì´)
        enhanced_result = quick_food_detection(test_image_path, use_enhanced=True)
        print(f"ğŸš€ Enhanced íƒì§€: {enhanced_result.get('statistics', {}).get('total_objects', 0)}ê°œ")
        
        # Enhanced íƒì§€ (OCR í¬í•¨)
        if test_ocr_text:
            enhanced_ocr_result = quick_food_detection(test_image_path, use_enhanced=True, ocr_text=test_ocr_text)
            print(f"ğŸ“„ Enhanced + OCR: {enhanced_ocr_result.get('method', 'unknown')}")
        
        return {
            "basic": basic_result,
            "enhanced": enhanced_result,
            "enhanced_ocr": enhanced_ocr_result if test_ocr_text else None
        }
    else:
        print("\nâš ï¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ê°€ ì—†ì–´ ì´ë¯¸ì§€ ë¶„ì„ í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€")
        return {"brand_test": "completed"}

def print_enhanced_summary(result):
    """Enhanced íƒì§€ ê²°ê³¼ ìƒì„¸ ìš”ì•½ - ê¸°ì¡´ ìœ ì§€"""
    if not result.get("success"):
        print(f"âŒ íƒì§€ ì‹¤íŒ¨: {result.get('error', 'ì•Œìˆ˜ì—†ëŠ” ì˜¤ë¥˜')}")
        return
    
    detections = result.get("detections", [])
    stats = result.get("statistics", {})
    enhancement_info = result.get("enhancement_info", {})
    method = result.get("method", "unknown")
    
    print("=" * 60)
    print("ğŸ“‹ Enhanced YOLO íƒì§€ ê²°ê³¼ ìƒì„¸ ìš”ì•½")
    print("=" * 60)
    print(f"ğŸ”¬ ë¶„ì„ ë°©ë²•: {method}")
    print(f"ğŸ“Š íƒì§€ ë‹¨ê³„: {result.get('analysis_stage', 'unknown')}")
    print(f"ğŸ” ì´ íƒì§€ ê°ì²´: {stats.get('total_objects', 0)}ê°œ")
    print(f"ğŸ ê³ ìœ  ìŒì‹ ì¢…ë¥˜: {stats.get('unique_classes', 0)}ê°œ")
    
    if enhancement_info:
        print(f"\nğŸš€ Enhancement í†µê³„:")
        print(f"   ğŸ¨ ìƒ‰ìƒ ê¸°ë°˜ ë³´ì™„: {enhancement_info.get('enhanced_predictions', 0)}ê°œ")
        print(f"   ğŸ”§ ë³´ì • ê·œì¹™ ì ìš©: {enhancement_info.get('corrections_applied', 0)}ê°œ")
        print(f"   ğŸ“¦ ì‚¬ìš©ëœ ëª¨ë¸: {', '.join(enhancement_info.get('models_used', []))}")
    
    # ë¸Œëœë“œ ì •ë³´
    brand_info = result.get('brand_info')
    if brand_info:
        print(f"\nğŸ·ï¸ ë¸Œëœë“œ ì •ë³´:")
        print(f"   ë¸Œëœë“œ: {brand_info.get('brand', 'unknown')}")
        print(f"   ì¹´í…Œê³ ë¦¬: {brand_info.get('category', 'unknown')}")
    
    # í´ë˜ìŠ¤ë³„ ê°œìˆ˜
    class_counts = stats.get('class_counts', {})
    if class_counts:
        print("\nğŸ“Š íƒì§€ëœ ìŒì‹ë³„ ê°œìˆ˜:")
        for food, count in sorted(class_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"   {food}: {count}ê°œ")
    
    # ë³´ì • ì ìš©ëœ í•­ëª©ë“¤
    corrected_items = [d for d in detections if d.get('correction_applied')]
    if corrected_items:
        print(f"\nğŸ”§ ë³´ì • ì ìš© í•­ëª©:")
        for item in corrected_items:
            original = item.get('original_class', 'unknown')
            corrected = item.get('class', 'unknown')
            reason = item.get('correction_reason', 'unknown')
            print(f"   {original} â†’ {corrected} ({reason})")
    
    # ì‹ ë¢°ë„ í†µê³„
    conf_stats = stats.get('confidence_stats', {})
    if conf_stats:
        print(f"\nğŸ“ˆ ì‹ ë¢°ë„ í†µê³„:")
        print(f"   í‰ê· : {conf_stats.get('average', 0):.3f}")
        print(f"   ìµœê³ : {conf_stats.get('max', 0):.3f}")
        print(f"   ìµœì €: {conf_stats.get('min', 0):.3f}")
    
    print("=" * 60)

def get_supported_features():
    """ì§€ì› ê¸°ëŠ¥ ëª©ë¡ ë°˜í™˜ - ê¸°ì¡´ ìœ ì§€"""
    return {
        "detection_features": [
            "3-Model Ensemble Detection",
            "Smart Color Analysis", 
            "Generalized Correction Rules",  # ğŸ†• ì¼ë°˜í™”ëœ ë³´ì •
            "Food-Only Filtering",
            "IoU-based Deduplication"
        ],
        "brand_recognition": {
            "categories": list(BRAND_MAPPINGS.keys()),
            "total_brands": sum(len(data["brands"]) for data in BRAND_MAPPINGS.values()),
            "ocr_priority": True
        },
        "correction_rules": {  # ğŸ†• ë³´ì • ê·œì¹™ ì •ë³´
            "total_rules": len(CORRECTION_RULES),
            "rule_names": [rule.get("name", "unnamed") for rule in CORRECTION_RULES]
        },
        "analysis_stages": [
            "1. OCR Brand Recognition (Priority)",
            "2. Ensemble Detection + Color Analysis", 
            "3. Generalized Corrections",  # ğŸ†• ì¼ë°˜í™”ëœ ë³´ì •
            "4. Food Filtering",
            "5. Statistical Analysis"
        ],
        "supported_models": ["yolo11s.pt", "best.pt", "best_friged.pt"],
        "color_ranges": len(COLOR_RANGES),
        "korean_mappings": len(KOREAN_CLASS_MAPPING)
    }

if __name__ == "__main__":
    print("ğŸš€ Enhanced YOLO Detector - ê°œì„ ëœ ë²„ì „ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # ê¸°ëŠ¥ ì •ë³´ ì¶œë ¥
    features = get_supported_features()
    print("ğŸ“‹ ì§€ì› ê¸°ëŠ¥:")
    for feature in features["detection_features"]:
        print(f"   âœ… {feature}")
    
    print(f"\nğŸ·ï¸ ë¸Œëœë“œ ì¸ì‹: {features['brand_recognition']['total_brands']}ê°œ ë¸Œëœë“œ")
    print(f"ğŸ”§ ë³´ì • ê·œì¹™: {features['correction_rules']['total_rules']}ê°œ")
    print(f"ğŸ¨ ìƒ‰ìƒ ë²”ìœ„: {features['color_ranges']}ê°œ")
    print(f"ğŸ‡°ğŸ‡· í•œêµ­ì–´ ë§¤í•‘: {features['korean_mappings']}ê°œ")
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_results = test_enhanced_detector(
        test_image_path="orange.jpeg", 
        test_ocr_text="ìŠ¤íƒ€ë²…ìŠ¤ ì•„ë©”ë¦¬ì¹´ë…¸"
    )
    
    if test_results and "enhanced" in test_results:
        print("\nğŸš€ Enhanced íƒì§€ ê²°ê³¼:")
        print_enhanced_summary(test_results["enhanced"])
    
    print("\nâœ… Enhanced YOLO Detector í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
